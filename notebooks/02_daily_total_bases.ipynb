{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1880ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Add src to sys.path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import importlib\n",
    "import data_utils, model_utils, shap_utils, ensemble_utils\n",
    "importlib.reload(data_utils)\n",
    "importlib.reload(model_utils)\n",
    "importlib.reload(shap_utils)\n",
    "importlib.reload(ensemble_utils)\n",
    "\n",
    "from config import TARGET_VAR, set_seed, device\n",
    "from data_utils import load_batting_years, build_feature_dataset\n",
    "from model_utils import PlayerMLP\n",
    "from shap_utils import explain_shap, get_top_shap_features\n",
    "from ensemble_utils import load_ensemble_and_predict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "set_seed()\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f3fab",
   "metadata": {},
   "source": [
    "# dont run every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pybaseball import statcast\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def fetch_statcast_data(start_date=\"2023-03-30\", end_date=None, verbose=True):\n",
    "    if end_date is None:\n",
    "        end_date = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Downloading Statcast data from {start_date} to {end_date}...\")\n",
    "    all_data = []\n",
    "\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Step through in 7-day increments to avoid malformed responses\n",
    "    delta = timedelta(days=7)\n",
    "    current = start\n",
    "    chunk_count = 0\n",
    "    skipped = 0\n",
    "\n",
    "    while current < end:\n",
    "        next_day = min(current + delta, end)\n",
    "        print(f\"Fetching {current.date()} → {next_day.date()}...\")\n",
    "        try:\n",
    "            if verbose:\n",
    "                chunk = statcast(start_dt=current.strftime('%Y-%m-%d'),\n",
    "                                 end_dt=next_day.strftime('%Y-%m-%d'))\n",
    "            else:\n",
    "                with redirect_stdout(StringIO()), redirect_stderr(StringIO()):\n",
    "                    chunk = statcast(start_dt=current.strftime('%Y-%m-%d'),\n",
    "                                     end_dt=next_day.strftime('%Y-%m-%d'))\n",
    "            all_data.append(chunk)\n",
    "            chunk_count += 1\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            print(f\"{current.date()} → {next_day.date()} failed: {e}\")\n",
    "        current = next_day\n",
    "        if verbose:\n",
    "            time.sleep(1)\n",
    "\n",
    "    df_statcast = pd.concat(all_data, ignore_index=True)\n",
    "    df_statcast = df_statcast[df_statcast['events'].notna()]\n",
    "    print(f\"Download complete: {len(df_statcast)} events from {chunk_count} chunks ({skipped} skipped)\")\n",
    "    return df_statcast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_bases(row):\n",
    "    if row['events'] == 'single': return 1\n",
    "    elif row['events'] == 'double': return 2\n",
    "    elif row['events'] == 'triple': return 3\n",
    "    elif row['events'] == 'home_run': return 4\n",
    "    else: return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898470a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ad01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pitcher_rolling_features(df_statcast, rolling_windows=[1, 3, 7, 14, 30], min_bf=10):\n",
    "    \"\"\"\n",
    "    Builds rolling stats for each pitcher prior to each game date.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_statcast: raw Statcast DataFrame\n",
    "    - rolling_windows: list of day ranges for feature aggregation\n",
    "    - min_bf: minimum batters faced in the rolling window\n",
    "\n",
    "    Returns:\n",
    "    - pitcher_features: DataFrame with pitcher-date level features\n",
    "    \"\"\"\n",
    "    df_statcast['game_date'] = pd.to_datetime(df_statcast['game_date'])\n",
    "    dates = sorted(df_statcast['game_date'].unique())\n",
    "    pitcher_rows = []\n",
    "\n",
    "    for i in tqdm(range(max(rolling_windows), len(dates)), desc=\"Processing pitcher dates\"):\n",
    "        current_date = dates[i]\n",
    "        prev_df = df_statcast[df_statcast['game_date'] < current_date]\n",
    "        today_df = df_statcast[df_statcast['game_date'] == current_date]\n",
    "\n",
    "        for pitcher in today_df['pitcher'].unique():\n",
    "            row = {'pitcher': pitcher, 'game_date': current_date}\n",
    "            for w in rolling_windows:\n",
    "                window_df = prev_df[\n",
    "                    (prev_df['game_date'] >= current_date - pd.Timedelta(days=w)) &\n",
    "                    (prev_df['pitcher'] == pitcher)\n",
    "                ]\n",
    "                if len(window_df) < min_bf:\n",
    "                    continue\n",
    "\n",
    "                prefix = f\"{w}d_\"\n",
    "                pa = len(window_df)\n",
    "                row[prefix + 'bf'] = pa\n",
    "                row[prefix + 'avg_exit_velo'] = window_df['launch_speed'].mean()\n",
    "                row[prefix + 'hard_hit_rate'] = (window_df['launch_speed'] >= 95).sum() / (pa + 1e-5)\n",
    "                row[prefix + 'hr_allowed'] = (window_df['events'] == 'home_run').sum()\n",
    "                row[prefix + 'bb_rate'] = (window_df['events'] == 'walk').sum() / (pa + 1e-5)\n",
    "                row[prefix + 'k_rate'] = (window_df['events'] == 'strikeout').sum() / (pa + 1e-5)\n",
    "\n",
    "            pitcher_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(pitcher_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c0d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pitcher_context_features(df_games, df_statcast):\n",
    "    \"\"\"\n",
    "    Enhances df_games with contextual pitcher features:\n",
    "    - p_throws (pitcher handedness)\n",
    "    - stand (batter handedness)\n",
    "    - handed_matchup (e.g., L_vs_R)\n",
    "    - is_home flag based on inning_topbot\n",
    "    \n",
    "    Returns:\n",
    "        df_games with additional features\n",
    "    \"\"\"\n",
    "    # Pitcher handedness\n",
    "    pitcher_handed = df_statcast[['game_date', 'player_name', 'pitcher', 'p_throws']].drop_duplicates()\n",
    "\n",
    "    # Batter handedness\n",
    "    batter_handed = df_statcast[['game_date', 'player_name', 'stand']].drop_duplicates()\n",
    "\n",
    "    # Venue/home flag\n",
    "    venue_info = df_statcast[['game_date', 'player_name', 'inning_topbot']].drop_duplicates()\n",
    "    venue_info['is_home'] = venue_info['inning_topbot'] == 'Bot'\n",
    "\n",
    "    # Merge all into df_games\n",
    "    df_games = df_games.merge(pitcher_handed, on=['game_date', 'player_name'], how='left')\n",
    "    df_games = df_games.merge(batter_handed, on=['game_date', 'player_name'], how='left')\n",
    "    df_games = df_games.merge(venue_info[['game_date', 'player_name', 'is_home']], on=['game_date', 'player_name'], how='left')\n",
    "\n",
    "    # Create matchup feature (e.g., L_vs_R)\n",
    "    df_games['handed_matchup'] = df_games['stand'] + '_vs_' + df_games['p_throws']\n",
    "\n",
    "    # One-hot encode matchup\n",
    "    matchup_dummies = pd.get_dummies(df_games['handed_matchup'], prefix='matchup')\n",
    "    df_games = pd.concat([df_games, matchup_dummies], axis=1)\n",
    "\n",
    "    return df_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def build_rolling_feature_dataset(df_statcast, rolling_windows=[1, 3, 7, 14, 30], min_pa=5):\n",
    "    dates = sorted(df_statcast['game_date'].unique())\n",
    "    player_games = []\n",
    "    def process_day(i):\n",
    "        current_date = dates[i]\n",
    "        today_games = df_statcast[df_statcast['game_date'] == current_date]\n",
    "        today_players = today_games['player_name'].unique()\n",
    "        rows = []\n",
    "\n",
    "        for player in today_players:\n",
    "            row = {\n",
    "                'player_name': player,\n",
    "                'game_date': current_date,\n",
    "                'total_bases': today_games[today_games['player_name'] == player]['total_bases'].sum()\n",
    "            }\n",
    "\n",
    "            for w in rolling_windows:\n",
    "                window_data = df_statcast[\n",
    "                    (df_statcast['game_date'] >= dates[i - w]) &\n",
    "                    (df_statcast['game_date'] < dates[i]) &\n",
    "                    (df_statcast['player_name'] == player)\n",
    "                ]\n",
    "\n",
    "                if len(window_data) < min_pa:\n",
    "                    continue\n",
    "\n",
    "                prefix = f\"{w}d_\"\n",
    "                pa = len(window_data)\n",
    "                at_bats = window_data['at_bat_number'].count()\n",
    "\n",
    "                row[prefix + 'pa'] = pa\n",
    "                row[prefix + 'launch_speed'] = window_data['launch_speed'].mean()\n",
    "                row[prefix + 'launch_angle'] = window_data['launch_angle'].mean()\n",
    "                row[prefix + 'woba'] = window_data['woba_value'].sum() / (pa + 1e-5)\n",
    "                row[prefix + 'slg'] = window_data['total_bases'].sum() / (at_bats + 1e-5)\n",
    "                row[prefix + 'hard_hits'] = (window_data['launch_speed'] >= 95).sum()\n",
    "                row[prefix + 'hard_hit_rate'] = row[prefix + 'hard_hits'] / pa\n",
    "                row[prefix + 'barrels'] = window_data['estimated_ba_using_speedangle'].notna().sum()\n",
    "                row[prefix + 'barrel_rate'] = row[prefix + 'barrels'] / pa\n",
    "                row[prefix + 'singles'] = (window_data['events'] == 'single').sum()\n",
    "                row[prefix + 'doubles'] = (window_data['events'] == 'double').sum()\n",
    "                row[prefix + 'triples'] = (window_data['events'] == 'triple').sum()\n",
    "                row[prefix + 'hr'] = (window_data['events'] == 'home_run').sum()\n",
    "                row[prefix + 'walks'] = (window_data['events'] == 'walk').sum()\n",
    "                row[prefix + 'strikeouts'] = (window_data['events'] == 'strikeout').sum()\n",
    "                row[prefix + 'hbp'] = (window_data['events'] == 'hit_by_pitch').sum()\n",
    "                row[prefix + 'tb_per_pa'] = window_data['total_bases'].sum() / (pa + 1e-5)\n",
    "                row[prefix + 'hr_per_pa'] = row[prefix + 'hr'] / (pa + 1e-5)\n",
    "                row[prefix + 'bb_rate'] = row[prefix + 'walks'] / (pa + 1e-5)\n",
    "                row[prefix + 'k_rate'] = row[prefix + 'strikeouts'] / (pa + 1e-5)\n",
    "                row[prefix + 'batted_balls'] = window_data['bb_type'].count()\n",
    "                row[prefix + 'events'] = window_data['events'].count()\n",
    "\n",
    "            rows.append(row)\n",
    "        return rows\n",
    "\n",
    "    print(\"Building rolling feature dataset in parallel...\")\n",
    "    all_rows = Parallel(n_jobs=-1)(\n",
    "        delayed(process_day)(i) for i in tqdm(range(max(rolling_windows), len(dates) - 1))\n",
    "    )\n",
    "    player_games = [row for sublist in all_rows for row in sublist]\n",
    "    df_games = pd.DataFrame(player_games)\n",
    "    df_games = df_games.dropna()\n",
    "\n",
    "    venue_data = df_statcast[['game_date', 'player_name', 'home_team', 'away_team', 'inning_topbot']].drop_duplicates()\n",
    "    df_games = df_games.merge(venue_data, on=['game_date', 'player_name'], how='left')\n",
    "    df_games['is_home'] = df_games['inning_topbot'] == 'Bot'\n",
    "\n",
    "    print(\"✅ Built enhanced rolling feature dataset:\", df_games.shape)\n",
    "    return df_games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8482fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this once\n",
    "df_statcast = fetch_statcast_data(start_date=\"2000-03-01\", verbose=False)\n",
    "df_statcast['game_date'] = pd.to_datetime(df_statcast['game_date'])\n",
    "df_statcast['total_bases'] = df_statcast.apply(compute_total_bases, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5296ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statcast.to_parquet(\"../data/statcast_2000_2025.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48138988",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "team_code_to_venue = {\n",
    "    \"ARI\": \"Chase Field\",\n",
    "    \"ATL\": \"Truist Park\",\n",
    "    \"BAL\": \"Oriole Park at Camden Yards\",\n",
    "    \"BOS\": \"Fenway Park\",\n",
    "    \"CHC\": \"Wrigley Field\",\n",
    "    \"CIN\": \"Great American Ball Park\",\n",
    "    \"CLE\": \"Progressive Field\",\n",
    "    \"COL\": \"Coors Field\",\n",
    "    \"CWS\": \"Rate Field\",\n",
    "    \"DET\": \"Comerica Park\",\n",
    "    \"HOU\": \"Daikin Park\",\n",
    "    \"KC\": \"Kauffman Stadium\",\n",
    "    \"LAA\": \"Angel Stadium\",\n",
    "    \"LAD\": \"Dodger Stadium\",\n",
    "    \"MIA\": \"loanDepot park\",\n",
    "    \"MIL\": \"American Family Field\",\n",
    "    \"MIN\": \"Target Field\",\n",
    "    \"NYM\": \"Citi Field\",\n",
    "    \"NYY\": \"Yankee Stadium\",\n",
    "    \"OAK\": \"Oakland Coliseum\",  # Not in park factors but okay\n",
    "    \"PHI\": \"Citizens Bank Park\",\n",
    "    \"PIT\": \"PNC Park\",\n",
    "    \"SD\": \"Petco Park\",\n",
    "    \"SEA\": \"T-Mobile Park\",\n",
    "    \"SF\": \"Oracle Park\",\n",
    "    \"STL\": \"Busch Stadium\",\n",
    "    \"TB\": \"Tropicana Field\",    # Not in park factors but okay\n",
    "    \"TEX\": \"Globe Life Field\",\n",
    "    \"TOR\": \"Rogers Centre\",\n",
    "    \"WSH\": \"Nationals Park\"\n",
    "}\n",
    "\n",
    "# Add venue column to df_statcast for merging\n",
    "df_statcast['venue'] = df_statcast['home_team'].map(team_code_to_venue)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de82436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statcast.to_parquet(\"../data/statcast_2000_2025_venues.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games.to_parquet(\"df_games_partial.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f1093",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = pd.read_parquet(\"df_games_partial.parquet\")\n",
    "print(list(df_games.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada37baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games['is_home'] = df_games['is_home_x']\n",
    "df_games.drop(columns=['is_home_x', 'is_home_y'], inplace=True)\n",
    "print(list(df_games.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce899589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games[\"team_name\"] = np.where(df_games[\"is_home\"], df_games[\"home_team\"], df_games[\"away_team\"])\n",
    "df_games[\"opponent_team\"] = np.where(df_games[\"is_home\"], df_games[\"away_team\"], df_games[\"home_team\"])\n",
    "\n",
    "park_factors = pd.read_csv(\"../data/park_factors.csv\")\n",
    "df_games = df_games.merge(park_factors, left_on=\"home_team\", right_on=\"venue\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60cfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = build_rolling_feature_dataset(df_statcast)\n",
    "df_games = add_pitcher_context_features(df_games, df_statcast)  # Handedness, venue, etc.\n",
    "pitcher_df = build_pitcher_rolling_features(df_statcast)        # Pitcher rolling stats\n",
    "df_games = df_games.merge(pitcher_df, on=[\"game_date\", \"pitcher\"], how=\"left\")  # Merge in\n",
    "\n",
    "df_games[\"team_name\"] = np.where(df_games[\"is_home\"], df_games[\"home_team\"], df_games[\"away_team\"])\n",
    "df_games[\"opponent_team\"] = np.where(df_games[\"is_home\"], df_games[\"away_team\"], df_games[\"home_team\"])\n",
    "\n",
    "park_factors = pd.read_csv(\"../data/park_factors.csv\")\n",
    "df_games = df_games.merge(park_factors, left_on=\"home_team\", right_on=\"venue\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f502f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games.to_parquet(\"../data/rolling_features_2000_2025.parquet\", index=False)\n",
    "\n",
    "df_games.to_csv(\"../data/rolling_features_2000_2025.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66c0ed",
   "metadata": {},
   "source": [
    "# run every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6762c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_games = pd.read_parquet(\"../data/rolling_features_2000_2025.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c35c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_features = [col for col in numeric_features if any(prefix in col for prefix in ['1d_', '3d_', '7d_', '14d_', '30d_'])]\n",
    "for feature in rolling_features:\n",
    "    median_value = df_games[feature].median()\n",
    "    df_games[feature] = df_games[feature].fillna(median_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5857bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ParkFactor' in df_games.columns:\n",
    "    df_games['ParkFactor'] = df_games['ParkFactor'].fillna(1.0)  # neutral value\n",
    "\n",
    "for col in ['wOBACon', 'xBACON', 'xwOBACon']:\n",
    "    if col in df_games.columns:\n",
    "        league_avg = df_games[col].mean()\n",
    "        df_games[col] = df_games[col].fillna(league_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f4108d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14d_avg_exit_velo        0\n",
      "14d_barrel_rate          0\n",
      "14d_barrels              0\n",
      "14d_batted_balls         0\n",
      "14d_bb_rate_x            0\n",
      "                     ...  \n",
      "ParkFactor               0\n",
      "pitcher                  0\n",
      "wOBACon              14732\n",
      "xBACON               14732\n",
      "xwOBACon             14732\n",
      "Length: 146, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_games[numeric_features].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1349d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['wOBACon', 'xBACON', 'xwOBACon', 'BACON']:\n",
    "    if col in df_games.columns:\n",
    "        league_avg = df_games[col].mean()\n",
    "        if pd.isna(league_avg):\n",
    "            league_avg = 0.0  # or a neutral value\n",
    "        df_games[col] = df_games[col].fillna(league_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7788e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14d_avg_exit_velo    0\n",
      "14d_barrel_rate      0\n",
      "14d_barrels          0\n",
      "14d_batted_balls     0\n",
      "14d_bb_rate_x        0\n",
      "                    ..\n",
      "ParkFactor           0\n",
      "pitcher              0\n",
      "wOBACon              0\n",
      "xBACON               0\n",
      "xwOBACon             0\n",
      "Length: 146, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_games[numeric_features].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbbd7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VAR = \"total_bases\"\n",
    "search_dir = f\"{TARGET_VAR}_gridsearch_chunks\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d39af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['player_name', 'game_date', 'total_bases']\n",
    "features = df_games.columns.difference(exclude_cols).tolist()\n",
    "numeric_features = df_games[features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "\n",
    "X = df_games[numeric_features].values\n",
    "y = df_games['total_bases'].values\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split\n",
    "X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tensors for use in training\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float32).view(-1, 1)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float32).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10ab4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_model_with_valsplit(config):\n",
    "    hidden_dims, lr, epochs, batch_size, dropout, val_split, activation, scheduler_type = config\n",
    "\n",
    "    model = PlayerMLP(\n",
    "        input_dim=X_train.shape[1],\n",
    "        hidden_dims=hidden_dims,\n",
    "        dropout=dropout,\n",
    "        activation=activation\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "    if scheduler_type == 'cosine':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=10, T_mult=2, eta_min=lr * 0.1\n",
    "        )\n",
    "    elif scheduler_type == 'plateau':\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=5, verbose=False\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "        X_train.cpu(), y_train.cpu(), test_size=val_split, random_state=42\n",
    "    )\n",
    "    X_subtrain, X_val = X_subtrain.to(device), X_val.to(device)\n",
    "    y_subtrain, y_val = y_subtrain.to(device), y_val.to(device)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        torch.utils.data.TensorDataset(X_subtrain, y_subtrain),\n",
    "        batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    patience = 5\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = loss_fn(val_pred, y_val).item()\n",
    "            if scheduler_type == 'plateau':\n",
    "                scheduler.step(val_loss)  # for ReduceLROnPlateau, pass val_loss\n",
    "            elif scheduler_type == 'cosine':\n",
    "                scheduler.step(epoch)     # for CosineAnnealing, pass epoch\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                break\n",
    "\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test.to(device)).cpu().numpy().flatten()\n",
    "        y_true = y_test.cpu().numpy().flatten()\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    \n",
    "    model = model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return model, (config, mae, rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b179ddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "import itertools\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba5906ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 237600 total configs into 476 chunks (size=500)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid\n",
    "hidden_dim_options = [\n",
    "    [64, 32], [128, 64], [256, 128], [512, 256],\n",
    "    [64, 32, 16], [128, 64, 32], [256, 128, 64], [512, 256, 128],\n",
    "    [64, 32, 16, 8], [128, 64, 32, 16], [256, 128, 64, 32], [128, 64, 32, 16, 8],\n",
    "    [64, 32, 16, 8, 4], [128, 64, 32, 16, 8], [256, 128, 64, 32, 16],\n",
    "    [512, 256, 128, 64], [1024, 512, 256],\n",
    "    [64, 32, 16, 8, 4, 2], [128, 64, 32, 16, 8, 4], [256, 128, 64, 32, 16, 8],\n",
    "    [512, 256, 128, 64, 32], [1024, 512, 256, 128],\n",
    "]\n",
    "lr_options = [0.01, 0.005, 0.001, 0.0005, 0.0001]\n",
    "epoch_options = [50, 100, 150]\n",
    "batch_size_options = [32, 64, 128]\n",
    "dropout_options = [0.2, 0.3, 0.4, 0.5]\n",
    "val_split_options = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "activation_options = ['relu', 'gelu', 'tanh', 'leaky_relu', 'sigmoid', 'swish']\n",
    "scheduler_options = ['plateau', 'cosine']\n",
    "random.seed(42)\n",
    "\n",
    "# Generate and shuffle grid\n",
    "grid_configs = list(itertools.product(\n",
    "    hidden_dim_options, lr_options, epoch_options,\n",
    "    batch_size_options, dropout_options, val_split_options,\n",
    "    activation_options, scheduler_options\n",
    "))\n",
    "# Chunk setup\n",
    "chunk_size = 500\n",
    "chunks = [grid_configs[i:i + chunk_size] for i in range(0, len(grid_configs), chunk_size)]\n",
    "print(f\"Split {len(grid_configs)} total configs into {len(chunks)} chunks (size={chunk_size})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c757ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for any NaNs in numeric features:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for any NaNs in numeric features:\")\n",
    "print(df_games[numeric_features].isnull().sum().sum())\n",
    "df_games = df_games.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37b338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Grid Search Chunk 23/476 with 500 configs...\n",
      "Saved top 5 from chunk 23 to total_bases_gridsearch_chunks/top5_chunk_23.pkl\n",
      "\n",
      "Running Grid Search Chunk 24/476 with 500 configs...\n",
      "Saved top 5 from chunk 24 to total_bases_gridsearch_chunks/top5_chunk_24.pkl\n",
      "\n",
      "Running Grid Search Chunk 25/476 with 500 configs...\n",
      "Saved top 5 from chunk 25 to total_bases_gridsearch_chunks/top5_chunk_25.pkl\n",
      "\n",
      "Running Grid Search Chunk 26/476 with 500 configs...\n",
      "Saved top 5 from chunk 26 to total_bases_gridsearch_chunks/top5_chunk_26.pkl\n",
      "\n",
      "Running Grid Search Chunk 27/476 with 500 configs...\n",
      "Saved top 5 from chunk 27 to total_bases_gridsearch_chunks/top5_chunk_27.pkl\n",
      "\n",
      "Running Grid Search Chunk 28/476 with 500 configs...\n",
      "Saved top 5 from chunk 28 to total_bases_gridsearch_chunks/top5_chunk_28.pkl\n",
      "\n",
      "Running Grid Search Chunk 29/476 with 500 configs...\n",
      "Saved top 5 from chunk 29 to total_bases_gridsearch_chunks/top5_chunk_29.pkl\n",
      "\n",
      "Running Grid Search Chunk 30/476 with 500 configs...\n",
      "Saved top 5 from chunk 30 to total_bases_gridsearch_chunks/top5_chunk_30.pkl\n",
      "\n",
      "Running Grid Search Chunk 31/476 with 500 configs...\n",
      "Saved top 5 from chunk 31 to total_bases_gridsearch_chunks/top5_chunk_31.pkl\n",
      "\n",
      "Running Grid Search Chunk 32/476 with 500 configs...\n",
      "Saved top 5 from chunk 32 to total_bases_gridsearch_chunks/top5_chunk_32.pkl\n",
      "\n",
      "Running Grid Search Chunk 33/476 with 500 configs...\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(f\"{TARGET_VAR}_gridsearch_chunks\", exist_ok=True)\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    chunk_id = idx + 22\n",
    "    print(f\"\\nRunning Grid Search Chunk {chunk_id+1}/{len(chunks)} with {len(chunk)} configs...\")\n",
    "\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(train_mlp_model_with_valsplit)(cfg)\n",
    "        for i, cfg in enumerate(chunk)\n",
    "    )\n",
    "\n",
    "    models = [res[0] for res in results if res[0] is not None]\n",
    "    metrics = [res[1] for res in results if res[0] is not None]\n",
    "    top5 = sorted(zip(models, metrics), key=lambda x: x[1][2])[:5]\n",
    "    top5_cpu = [(m.to('cpu'), metrics) for m, metrics in top5]\n",
    "\n",
    "\n",
    "    chunk_path = f\"{TARGET_VAR}_gridsearch_chunks/top5_chunk_{chunk_id+1}.pkl\"\n",
    "    with open(chunk_path, \"wb\") as f:\n",
    "        pickle.dump(top5_cpu, f)\n",
    "\n",
    "    print(f\"Saved top 5 from chunk {chunk_id+1} to {chunk_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_top = []\n",
    "for path in sorted(glob.glob(os.path.join(search_dir, \"top5_chunk_*.pkl\"))):\n",
    "    try:\n",
    "        with open(path, \"rb\") as f:\n",
    "            chunk_top5 = pickle.load(f)\n",
    "        combined_top.extend(chunk_top5)\n",
    "    except Exception as e1:\n",
    "        print(f\"Failed to load {path}: {e1}\")\n",
    "\n",
    "# Sort and print final top 5\n",
    "final_top5 = sorted(combined_top, key=lambda x: x[1][2])[:5]\n",
    "\n",
    "print(\"\\n✅ Final Top 5 Grid Search Results:\")\n",
    "for rank, (model, (cfg, mae, rmse)) in enumerate(final_top5, 1):\n",
    "    print(f\"Top {rank}: {cfg} → MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        'rank': i+1,\n",
    "        'hidden_dims': cfg[0],\n",
    "        'lr': cfg[1],\n",
    "        'epochs': cfg[2],\n",
    "        'batch_size': cfg[3],\n",
    "        'dropout': cfg[4],\n",
    "        'val_split': cfg[5],\n",
    "        'activation': cfg[6],\n",
    "        'scheduler': cfg[7],\n",
    "        'mae': mae,\n",
    "        'rmse': rmse\n",
    "    }\n",
    "    for i, (model, (cfg, mae, rmse)) in enumerate(final_top5)\n",
    "])\n",
    "summary_df.to_csv(f\"{TARGET_VAR}_top5_summary.csv\", index=False)\n",
    "print(f\"\\nSaved summary to {TARGET_VAR}_top5_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fe477",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = final_top5[0][0]\n",
    "torch.save(best_model.state_dict(), f\"{TARGET_VAR}_best_model.pt\")\n",
    "print(\"Best model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262ed3f",
   "metadata": {},
   "source": [
    "## load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_statcast = pd.read_parquet(\"../data/statcast_2000_2025.parquet\")\n",
    "df_statcast['game_date'] = pd.to_datetime(df_statcast['game_date'])\n",
    "df_statcast['total_bases'] = df_statcast.apply(compute_total_bases, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687c4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_code_to_venue = {\n",
    "    \"ARI\": \"Chase Field\",\n",
    "    \"ATL\": \"Truist Park\",\n",
    "    \"BAL\": \"Oriole Park at Camden Yards\",\n",
    "    \"BOS\": \"Fenway Park\",\n",
    "    \"CHC\": \"Wrigley Field\",\n",
    "    \"CIN\": \"Great American Ball Park\",\n",
    "    \"CLE\": \"Progressive Field\",\n",
    "    \"COL\": \"Coors Field\",\n",
    "    \"CWS\": \"Rate Field\",\n",
    "    \"DET\": \"Comerica Park\",\n",
    "    \"HOU\": \"Daikin Park\",\n",
    "    \"KC\": \"Kauffman Stadium\",\n",
    "    \"LAA\": \"Angel Stadium\",\n",
    "    \"LAD\": \"Dodger Stadium\",\n",
    "    \"MIA\": \"loanDepot park\",\n",
    "    \"MIL\": \"American Family Field\",\n",
    "    \"MIN\": \"Target Field\",\n",
    "    \"NYM\": \"Citi Field\",\n",
    "    \"NYY\": \"Yankee Stadium\",\n",
    "    \"OAK\": \"Oakland Coliseum\",  # Not in park factors but okay\n",
    "    \"PHI\": \"Citizens Bank Park\",\n",
    "    \"PIT\": \"PNC Park\",\n",
    "    \"SD\": \"Petco Park\",\n",
    "    \"SEA\": \"T-Mobile Park\",\n",
    "    \"SF\": \"Oracle Park\",\n",
    "    \"STL\": \"Busch Stadium\",\n",
    "    \"TB\": \"Tropicana Field\",    # Not in park factors but okay\n",
    "    \"TEX\": \"Globe Life Field\",\n",
    "    \"TOR\": \"Rogers Centre\",\n",
    "    \"WSH\": \"Nationals Park\"\n",
    "}\n",
    "\n",
    "# Add venue column to df_statcast for merging\n",
    "df_statcast['venue'] = df_statcast['home_team'].map(team_code_to_venue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c845d",
   "metadata": {},
   "source": [
    "## generate rolling features up to today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c79f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "today = pd.to_datetime(datetime.today().date())\n",
    "\n",
    "# Only use statcast data *before today* to avoid leakage\n",
    "df_statcast = pd.read_parquet(\"../data/statcast_2000_2025.parquet\")\n",
    "df_statcast['game_date'] = pd.to_datetime(df_statcast['game_date'])\n",
    "df_statcast = df_statcast[df_statcast['game_date'] < today]\n",
    "df_statcast['total_bases'] = df_statcast.apply(compute_total_bases, axis=1)\n",
    "\n",
    "# Use your existing function to build rolling features\n",
    "df_rolling = build_rolling_feature_dataset(df_statcast)\n",
    "df_today = df_rolling[df_rolling['game_date'] == df_rolling['game_date'].max()].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fccf13",
   "metadata": {},
   "source": [
    "## load model and assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import joblib\n",
    "\n",
    "# Load saved model, scaler, and feature list\n",
    "model = PlayerMLP(input_dim=512, hidden_dims=[128, 64], dropout=0.3, activation='relu')\n",
    "model.load_state_dict(torch.load(\"total_bases_best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "scaler = joblib.load(\"ensemble_engineered/scaler_total_bases.joblib\")\n",
    "features = joblib.load(\"ensemble_engineered/features_total_bases.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f63fd",
   "metadata": {},
   "source": [
    "## predict with saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f77c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_today = df_today[features].fillna(0).values\n",
    "X_today_scaled = scaler.transform(X_today)\n",
    "X_today_tensor = torch.tensor(X_today_scaled, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_today = model(X_today_tensor).cpu().numpy().flatten()\n",
    "\n",
    "df_today[\"Pred_TB\"] = y_pred_today\n",
    "df_today = df_today.sort_values(by=\"Pred_TB\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9b10b",
   "metadata": {},
   "source": [
    "## evaluate thresholds historically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['player_name', 'game_date', 'total_bases']\n",
    "features = df_rolling.columns.difference(exclude_cols).tolist()\n",
    "\n",
    "X = scaler.transform(df_rolling[features].fillna(0).values)\n",
    "y = df_rolling['total_bases'].values\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(torch.tensor(X, dtype=torch.float32)).cpu().numpy().flatten()\n",
    "\n",
    "df_rolling[\"Pred_TB\"] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(1.5, 4.1, 0.2)\n",
    "results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    df_rolling[\"Bet\"] = df_rolling[\"Pred_TB\"] > t\n",
    "    df_rolling[\"Won\"] = df_rolling[\"total_bases\"] > 1.5\n",
    "    subset = df_rolling[df_rolling[\"Bet\"]]\n",
    "\n",
    "    if len(subset) == 0:\n",
    "        continue\n",
    "\n",
    "    win_rate = subset[\"Won\"].mean()\n",
    "    profit = np.where(subset[\"Won\"], 10 * 0.91, -10)\n",
    "    roi = profit.sum() / (10 * len(subset))\n",
    "\n",
    "    results.append({\n",
    "        \"threshold\": t,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"roi\": roi,\n",
    "        \"bets\": len(subset)\n",
    "    })\n",
    "\n",
    "df_thresholds = pd.DataFrame(results)\n",
    "df_thresholds = df_thresholds.sort_values(by=\"roi\", ascending=False)\n",
    "print(df_thresholds.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e2de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_threshold = df_thresholds.iloc[0][\"threshold\"]\n",
    "df_today[\"Bet\"] = df_today[\"Pred_TB\"] > best_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac76ff",
   "metadata": {},
   "source": [
    "## add betting threshold today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd54bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_today[\"Bet\"] = df_today[\"Pred_TB\"] > best_threshold\n",
    "df_today[[\"player_name\", \"game_date\", \"Pred_TB\", \"Bet\"]].to_csv(\"today_predictions.csv\", index=False)\n",
    "\n",
    "print(df_today[[\"player_name\", \"Pred_TB\", \"Bet\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12468f4",
   "metadata": {},
   "source": [
    "# Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342838b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop games with very low plate appearances (e.g., pinch hits)\n",
    "df_games = df_games[df_games['plate_appearances'] >= 2].copy()\n",
    "\n",
    "# Drop nulls if any\n",
    "df_games = df_games.dropna()\n",
    "\n",
    "# Final feature list\n",
    "features = ['batted_balls', 'launch_speed', 'launch_angle', 'woba_value', 'event_count', 'plate_appearances']\n",
    "X = df_games[features].values\n",
    "y = df_games['total_bases'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c43984",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268b0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "model = PlayerMLP(X_train_tensor.shape[1], [64, 32], dropout=0.2, activation='relu').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(X_train_tensor.to(device))\n",
    "    loss = loss_fn(pred, y_train_tensor.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor.to(device)).cpu().numpy().flatten()\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred) ** 0.5\n",
    "\n",
    "print(f\"Daily MLP → MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8188372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set and compute binary classification\n",
    "preds = y_pred\n",
    "binary_true = (y_test > 1.5).astype(int)\n",
    "binary_pred = (preds > 1.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "print(\"Over 1.5 TB Accuracy:\", accuracy_score(binary_true, binary_pred))\n",
    "print(\"Precision:\", precision_score(binary_true, binary_pred))\n",
    "print(\"Recall:\", recall_score(binary_true, binary_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_names = df_games['player_name'].values\n",
    "X_train_names, X_test_names = train_test_split(player_names, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a09241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame({\n",
    "    \"Player\": X_test_names,\n",
    "    \"True_TB\": y_test,\n",
    "    \"Pred_TB\": y_pred\n",
    "})\n",
    "df_preds[\"Over_1.5\"] = df_preds[\"Pred_TB\"] > 1.5\n",
    "df_preds = df_preds.sort_values(by=\"Pred_TB\", ascending=False)\n",
    "print(df_preds.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a18d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds[\"Bet\"] = df_preds[\"Pred_TB\"] > 2.0\n",
    "df_preds[\"Won\"] = df_preds[\"True_TB\"] > 1.5\n",
    "\n",
    "bet_results = df_preds[df_preds[\"Bet\"]]\n",
    "win_rate = bet_results[\"Won\"].mean()\n",
    "print(f\"Win rate on high-confidence bets: {win_rate:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for threshold in [1.6, 1.8, 2.0, 2.2, 2.4, 2.6, 2.8, 3.0, 3.2, 3.4, 3.6, 3.8, 4.0, 5.0]:\n",
    "    df_preds[\"Bet\"] = df_preds[\"Pred_TB\"] > threshold\n",
    "    df_preds[\"Won\"] = df_preds[\"True_TB\"] > 1.5\n",
    "    win_rate = df_preds[df_preds[\"Bet\"]][\"Won\"].mean()\n",
    "    print(f\"Threshold {threshold:.1f} → Win Rate: {win_rate:.2%}\")\n",
    "    bet_results = df_preds[df_preds[\"Bet\"]]\n",
    "    bet_results[\"Profit\"] = np.where(bet_results[\"Won\"], 10 * 0.91, -10)\n",
    "    roi = bet_results[\"Profit\"].sum() / (10 * len(bet_results))\n",
    "    print(f\"Simulated ROI: {roi:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
